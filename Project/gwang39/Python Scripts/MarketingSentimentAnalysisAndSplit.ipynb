{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis And Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guan Yue Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Sentiment analysis classifier is built based on Sentiment 140 corpus, NLTK, and code from Laurent Luce's Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "Sentiment 140 corpus: http://help.sentiment140.com/for-students\n",
    "\n",
    "Laurent Luce's Blog: http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\n",
    "\n",
    "NLTK:https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations:\n",
    "- We assume training corpus and actual corpus are in the same domain and thus has the same meaning of words\n",
    "- Sarcasm and hyperbole cannot be easily identified by the sentiment analysis classifier\n",
    "- Reddit specific stop words are not considered\n",
    "- Punctuation is not fully considered into sentiment analysis\n",
    "- URLs, links, unnecessary contents cannot be excluded from the analysis\n",
    "- The program takes long time to run due to the training on sentiment 140 corpus\n",
    "- Due to computing constraints, only 10000 of the sentiment 140 corpus is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    negative = 0\n",
    "    neutral = 2\n",
    "    positive = 4\n",
    "\n",
    "\n",
    "def get_tweets(csvFile):\n",
    "    tweets = []\n",
    "    with open(csvFile, 'r') as f:\n",
    "        for line in f:\n",
    "            columns = line.split(',')\n",
    "            sentiment = Sentiment(int(columns[0].replace('\"','')))\n",
    "            tweet = columns[5]\n",
    "            filteredTweet = [e.lower().replace('\"','') for e in tweet.split() if len(e) >= 3]\n",
    "            tweets.append((filteredTweet, sentiment.name))\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "\n",
    "    return word_features\n",
    "\n",
    "\n",
    "training_tweets = get_tweets('training.1600000.processed.noemoticon.csv')\n",
    "random.shuffle(training_tweets)\n",
    "training_tweets = training_tweets[0:10000]\n",
    "test_tweets = get_tweets('testdata.manual.2009.06.14.csv')\n",
    "word_features = get_word_features(get_words_in_tweets(training_tweets))\n",
    "\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "def sentiment_analysis_classifier():\n",
    "    training_set = nltk.classify.apply_features(extract_features, training_tweets)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    print ('classifier accuracy:')\n",
    "    test_set = nltk.classify.apply_features(extract_features, test_tweets)\n",
    "    print ('\\t' + str(nltk.classify.accuracy(classifier, test_set)) + '\\n')\n",
    "    reddit_sentiment_classification(classifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_sentiment_classification(classifier):\n",
    "    print ('Sentiment Analysis Classifier - Reddit Comments: ')\n",
    "    file1 = open(\"finaldata_marketing_positive.txt\", \"w\",encoding='utf-8')\n",
    "    file2 = open(\"finaldata_marketing_negative.txt\", \"w\",encoding='utf-8')\n",
    "    positivecounts = 0\n",
    "    negativecounts = 0\n",
    "    \n",
    "    with open('finaldata_marketing.txt', encoding=\"utf8\") as f:\n",
    "        comments = f.readlines()\n",
    "        for comment in comments:\n",
    "            if classifier.classify(extract_features(comment.split())) == \"positive\":\n",
    "                file1.write(str(comment.replace('\\n','')) + '\\n')\n",
    "                positivecounts = positivecounts + 1\n",
    "            else:\n",
    "                file2.write(str(comment.replace('\\n','')) + '\\n')\n",
    "                negativecounts = negativecounts + 1\n",
    "        print(\"positive sentiment comment counts: \", positivecounts)\n",
    "        print(\"negative sentiment comment counts: \", negativecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy:\n",
      "\t0.5200803212851406\n",
      "\n",
      "Sentiment Analysis Classifier - Reddit Comments: \n",
      "positive sentiment comment counts:  14872\n",
      "negative sentiment comment counts:  4250\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
